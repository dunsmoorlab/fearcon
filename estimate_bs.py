from glob import glob
import os
from fc_config import *
from glm_timing import *
from preprocess_library import meta
from shutil import copytree, move

from argparse import ArgumentParser, RawTextHelpFormatter

from mvpa2.misc.fsl.base import FslGLMDesign, read_fsl_design
from mvpa2.datasets.mri import fmri_dataset, map2nifti

import numpy as np
import scipy.stats
from scipy.stats.mstats import zscore

def group_beta(phases=None, subs=None):
	#ptsd or controls
	if subs == 'p':
		subs = p_sub_args
	elif subs == 'c':
		subs = sub_args
	elif subs == 'no107':
		subs = no107
	elif subs == 'all':
		subs = all_sub_args

	for phase in phases:
		print(phase)
		#set the number of trials
		n_trials = beta_n_trials[phase]	
		print(n_trials)
		for sub in subs:
			print(sub)
			subj = meta(sub)
			#check to make sure all the timing files are there
			onsets = glob('%s/%s/model/GLM/onsets/%s/betaseries/trial**.txt'%(data_dir, subj.fsub, py_run_key[phase]))
			if len(onsets) != n_trials * len(subs): pop_beta_timing(phase=phase, subs=[sub])

			#check to make sure all of the fsf design files are there
			fsfs = glob('%s/%s/bold/%s/fsl_betas/%s_beta.fsf'%(data_dir, subj.fsub, phase2rundir[phase], py_run_key[phase]))
			if len(fsfs) != len(subs): populate_beta_fsf(phase=phase, subs=[sub])

			#now were ready to estimate the betas!
			estimate_betaseries(sub=sub,n_trials=n_trials,phase=phase,mask=None,no_zscore=True)


def pop_beta_timing(phase=None, subs=None):

	print('Populating beta timing files')

	if isinstance(phase, str): phase = [phase]

	for sub in subs:

		for run in phase:

			# if 'localizer' in run: glm_timing(sub, run).loc_blocks(beta=True)
			# if 'extinction_recall' in run: glm_timing(sub, run).betaseries(er_start=True)
			glm_timing(sub, run).betaseries()

def populate_beta_fsf(phase=None, subs=None):

	print('Populating beta design files')

	template = os.path.join(data_dir,'beta_templates',py_run_key[phase] + '_template.fsf')

	for sub in subs:

		subj = meta(sub)
	
		outdir = os.path.join(subj.bold_dir,phase2rundir[phase],'fsl_betas')
		if not os.path.exists(outdir): os.mkdir(outdir)

		outfile = os.path.join(outdir, py_run_key[phase] + '_beta.fsf')

		replacements = {'SUBJID':subj.fsub, 'RUNID':py_run_key[phase]}

		with open(template) as temp:

			with open(outfile,'w') as out:

				for line in temp:

					for src, target in replacements.items():
						line = line.replace(src, target)

					out.write(line)


def estimate_betaseries(sub=0,n_trials=0,phase=None,mask=None,no_zscore=True):

	subj = meta(sub)

	# find FSF files for this subject
	model_dir = os.path.join(subj.bold_dir,phase2rundir[phase],'fsl_betas')
	if not os.path.exists(model_dir): os.mkdir(model_dir)

	'''
	pattern = os.path.join(model_dir, 'fsf',
						   '{}_{}*.fsf'.format(args.model, args.subject))
	fsf_files = glob(pattern)
	if not fsf_files:
		raise IOError('No FSF files found matching: {}'.format(pattern))

	fsf_files.sort()

	log.start()
	'''

	fsf_files = [os.path.join(model_dir,'%s_beta.fsf'%(py_run_key[phase]))]

	# temporary subject directory for individual beta images
	'''
	out_dir = os.path.join(model_dir, 'beta', args.subject)
	log.run('mkdir -p {}'.format(out_dir))
	'''
	out_dir = model_dir
	

	beta_files = []
	for f in fsf_files:
		# use a feat utility to create the design matrix
		(base, ext) = os.path.splitext(f)
		name = os.path.basename(base)
		os.system('feat_model {}'.format(base))

		design = read_fsl_design(f)
		bold = design['feat_files']
		if not bold.endswith('.nii.gz'):
			bold += '.nii.gz'
		if not os.path.exists(bold):
			raise IOError('BOLD file not found: {}'.format(bold))
		
		# obtain individual trial estimates
		betaseries(base=base, out_dir=out_dir, n_trials=n_trials, mask=mask, no_zscore=no_zscore)

		# # get one file with estimates for each trial/stimulus		
		beta_file = os.path.join(out_dir, name + '.nii.gz')
		ev_files = []
		for i in range(n_trials):
			ev_files.append(os.path.join(out_dir, 'ev{:03d}.nii.gz'.format(i)))
		os.system('fslmerge -t {} {}'.format(beta_file, ' '.join(ev_files)))
		beta_files.append(beta_file)

		# remove temp files
		os.system('rm {}'.format(' '.join(ev_files)))
		os.system('rm {}*.{{con,png,ppm,frf,mat,min,trg}}'.format(base))


def betaseries(base=None,out_dir=None,n_trials=0,mask=None,no_zscore=True):
	s = """Estimate betaseries using LS-S regression.
	See Mumford et al. 2014 for details. 
	Specify the base for a design generated by FEAT. For example, if you
	have a .fsf file in mydesign.fsf, specify mydesign as the modelbase.
	The trial regressors are assumed to be in your original EVs (as opposed
	to the real EVs, which include for example temporal derivatives of the
	original EVs). The trials to model are assumed to be the first ones listed.
	For example, if 30 orig EVs are included in the model, and ntrials is set
	to 20, then the last 10 EVs are assumed to be modeling things other than
	the individual trials. The exception are temporal derivatives of the trial
	EVs, which are assumed to be interleaved with the original trial EVs.
	If derivatives are included in the model, they will be included as
	additional regressors. If --sep-derivs is included as an option, then the
	current trial derivative and other trial derivatives will be estimated
	separately.
	For unknown reasons, each trial image is z-scored over voxels. This
	means that the value o f a voxel in a given trial image will depend on
	things like the size of the mask and values at other voxels. For
	legacy purposes, for now that is still the default. To write raw
	betaseries estimates, use the --no-zscore flag.
	You may also specify confound regressors (defined in the fsf file under
	'confoundev_files'), which will be included as regressors of no interest.
	"""

	# parser = ArgumentParser(description="Estimate betaseries using LS-S regression (Mumford et al. 2014).")
	# parser.add_argument('modelbase', type=str,
	# 					help="path to model files, without file extension")
	# parser.add_argument('betadir', type=str,
	# 					help="path to directory in which to save betaseries image")
	# parser.add_argument('ntrials', type=int,
	# 					help="number of trials to be estimated")
	# parser.add_argument('-m', '--mask', type=str,
	# 					help="(optional) path to mask image, indicating included voxels")
	# parser.add_argument('-n', '--no-zscore', action="store_true",
	# 					help="do not z-score trial images over voxels")
	# parser.add_argument('-s', '--sep-derivs', action="store_true",
	# 					help="use separate trial and other derivative regressors")
	# args = parser.parse_args()

	fsffile = base + '.fsf'
	matfile = base + '.mat'
	betadir = out_dir
	n_trial = n_trials

	print("Loading design...")
	design = read_fsl_design(fsffile)
	desmat = FslGLMDesign(matfile)

	n_tp, n_evs = desmat.mat.shape

	# number of original regressors and all regressors including
	# derivatives
	n_orig = design['fmri(evs_orig)']

	# check which trial regressors have temporal derivatives
	isderiv = np.zeros(n_orig, dtype=bool)
	for i in range(n_orig):
		f = 'fmri(deriv_yn{:d})'.format(i+1)
		isderiv[i] = design[f]

	# check if derivatives are included for all trials
	n_trial_deriv = np.sum(isderiv)
	if n_trial_deriv == n_trial:
		deriv = True
	elif n_trial_deriv == 0:
		deriv = False
	else:
		raise ValueError('Must either include derivatives for all trials or none.')
		
	if deriv:
		# temporal derivatives are included. FEAT interleaves them with
		# the original ones, starting with the first original regressor
		n_trial_evs = n_trial * 2
		trial_evs = range(0, n_trial_evs, 2)
		deriv_evs = range(1, n_trial_evs, 2)
		print("Using derivatives of trial regressors.")
	else:
		# trial regressors are just the first N regressors
		n_trial_evs = n_trial
		trial_evs = range(0, n_trial)
		deriv_evs = []

	# find input bold data
	print("Loading data...")
	bold = design['feat_files']
	if not bold.endswith('.nii.gz'):
		bold += '.nii.gz'
	if not os.path.exists(bold):
		raise IOError('BOLD file not found: {}'.format(bold))

	#if args.mask is not None:
	if mask is not None:
		# user specified a mask
		if not mask.endswith('.nii.gz'):
			mask += '.nii.gz'
		if not os.path.exists(mask):
			raise IOError('Mask file not found: {}'.format(mask))
		data = fmri_dataset(bold, mask=mask)
	else:
		# load all voxels
		data = fmri_dataset(bold)

	# everything after the trial EVs is regressors of no interest
	dm_extra = desmat.mat[:,n_trial_evs:]

	# additional confound regressors
	if 'confoundev_files' in design:
		conf_file = design['confoundev_files']
		print("Loading confound file {}...".format(conf_file))
		dm_nuisance = np.loadtxt(conf_file)
	else:
		print("No confound file indicated. Including no confound regressors...")
		dm_nuisance = None

	# create a beta-forming vector for each trial
	print("Creating design matrices...")
	beta_maker = np.zeros((n_trial, n_tp))

	sep_derivs = False

	for i, ev in enumerate(trial_evs):
		# this trial
		if deriv and sep_derivs:
			# if using separate derivatives, include a dedicated regressor
			# for this trial
			dm_trial = np.hstack((desmat.mat[:,ev,np.newaxis],
								 desmat.mat[:,deriv_evs[i],np.newaxis]))
		else:
			# just the one regressor for this trial
			dm_trial = desmat.mat[:,ev,np.newaxis]

		# other trials, summed together
		other_trial_evs = [x for x in trial_evs if x != ev]
		if deriv:
			if args.sep_derivs:
				# only include derivatives except for this trial
				other_deriv_evs = [x for x in deriv_evs if x != deriv_evs[i]]
				dm_otherevs = np.hstack((
					np.sum(desmat.mat[:,other_trial_evs,np.newaxis],1),
					np.sum(desmat.mat[:,other_deriv_evs,np.newaxis],1)))
			else:
				# put all derivatives in one regressor
				dm_otherevs = np.hstack((
					np.sum(desmat.mat[:,other_trial_evs,np.newaxis],1),
					np.sum(desmat.mat[:,deriv_evs,np.newaxis],1)))
		else:
			# just one regressor for all other trials
			dm_otherevs = np.sum(desmat.mat[:,other_trial_evs,np.newaxis],1)

		# put together the design matrix
		if dm_nuisance is not None:
			dm_full = np.hstack((dm_trial, dm_otherevs, dm_nuisance, dm_extra))
		else:
			dm_full = np.hstack((dm_trial, dm_otherevs, dm_extra))
		s = dm_full.shape
		dm_full = dm_full - np.kron(np.ones(s), np.mean(dm_full,0))[:s[0],:s[1]]
		dm_full = np.hstack((dm_full, np.ones((n_tp,1))))

		# calculate beta-forming vector
		beta_maker_loop = np.linalg.pinv(dm_full)
		beta_maker[i,:] = beta_maker_loop[0,:]

	print("Estimating model...")
	# this uses Jeanette's trick of extracting the beta-forming vector for each
	# trial and putting them together, which allows estimation for all trials
	# at once
	glm_res_full = np.dot(beta_maker, data.samples)


	# map the data into images and save to betaseries directory
	for i in range(len(glm_res_full)):
		if no_zscore:
			ni = map2nifti(data, glm_res_full[i])
		else:
			outdata = zscore(glm_res_full[i])
			ni = map2nifti(data, data=outdata)
		ni.to_filename(os.path.join(betadir,
									'ev{:03d}.nii.gz'.format(i)))


def clean_old_betas(p=False):
	if p: subs = working_subs
	else: subs = sub_args
	
	for sub in subs:

		subj = meta(sub)

		for phase in ['localizer_1','localizer_2']:

			rundir = subj.bold_dir + phase2rundir[phase]

			target = os.path.join(rundir,'old_betas')

			os.mkdir(target)

			move(os.path.join(rundir,'ls-s_betas'),target)

			move(os.path.join(rundir,'new_ls-s_betas'),target)